# 环境
我的环境是：python3.5 + scrapy 2.0.0
# 爬取内容和思路
爬取内容：网页版微博热搜的关键词，链接，以及导语，即简要概述热搜内容的一小段话  

思路：  
1. 对于热搜链接：通过热搜关键词所在标签的属性再加上前缀即可（如图1）
2. 对于关键词：进入关键词所在链接，一般会有一个如图2所示的位置，根据标签解析出内容；如果没有，存入“无”
3. 对于导语：也是在关键词链接里边（如图3），通过解析获得；如果没有，爬取页面中的一条微博截取
4. 对于推荐类（如图4）：一般是广告之类，不在所爬取范围之内，可以在提取关键词链接时，通过标签最后位置是否为 **“荐”** 进行一个过滤
5. 关于文件保存，先将所爬取内容根据 **关键词，导语，链接** 的方式写入本地 txt 
6. 关于邮箱发送，在 pipelines 文件中重写 close_spider 函数，将保存在本地的 txt 文件发送给你想要发送的邮箱
# 不足
对于反爬虫的措施有点少，爬取数目过多之后，就会让你登陆，关于关键词的抓取不够
